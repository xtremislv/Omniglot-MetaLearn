{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fZJ_Hv8Uqoil"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Varad Shinde \\\\\n",
        "12141730\n"
      ],
      "metadata": {
        "id": "Oma2ZStyqpQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSsY1Jw7pwZc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "2NGBSeo0L6Vu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta learning parameters.\n",
        "\n",
        "N = 5\n",
        "support_size = 1\n",
        "query_size = 15\n",
        "meta_inner_lr = 0.4\n",
        "meta_outer_lr = 0.001"
      ],
      "metadata": {
        "id": "ZU6nY6ZPDJla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(0.5, 0.5)\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.Omniglot('./data/omniglot/', download = True, background = True, transform = transform)\n",
        "test_dataset = torchvision.datasets.Omniglot('./data/omniglot/', download = True, background = False, transform = transform)\n",
        "\n",
        "train_labels = np.repeat(np.arange(964), 20)\n",
        "test_labels = np.repeat(np.arange(659), 20)"
      ],
      "metadata": {
        "id": "pMXx6_Py9AhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c1708c-36a7-46eb-9cb9-40d10833eb63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchSampler(object):\n",
        "    '''\n",
        "    BatchSampler: yield a batch of indexes at each iteration.\n",
        "    __len__ returns the number of episodes per epoch (same as 'self.iterations').\n",
        "    '''\n",
        "\n",
        "    def __init__(self, labels, classes_per_it, support_size, query_size, iterations, batch_size):\n",
        "        super(BatchSampler, self).__init__()\n",
        "        self.labels = labels\n",
        "        self.classes_per_it = classes_per_it\n",
        "        self.support_size = support_size\n",
        "        self.query_size = query_size\n",
        "        self.iterations = iterations\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        '''\n",
        "        yield a batch of indexes\n",
        "        '''\n",
        "\n",
        "        for it in range(self.iterations):\n",
        "            total_batch_indexes = np.array([])\n",
        "\n",
        "            unique_labels = np.unique(self.labels)\n",
        "            for _ in range(self.batch_size):\n",
        "                random_classes = np.random.choice(unique_labels, size=N, replace=False)\n",
        "                support_seq = np.array([])\n",
        "                query_seq = np.array([])\n",
        "                for i, c in enumerate(random_classes):\n",
        "                    count = self.support_size + self.query_size\n",
        "                    random_indices = np.random.choice(np.arange(20*c, 20*c+20), size=count, replace=False)\n",
        "                    support_seq = np.concatenate((support_seq, random_indices[:self.support_size]))\n",
        "                    query_seq = np.concatenate((query_seq, random_indices[self.support_size:]))\n",
        "                np.random.shuffle(support_seq)\n",
        "                np.random.shuffle(query_seq)\n",
        "                total_batch_indexes = np.concatenate((total_batch_indexes, support_seq, query_seq))\n",
        "            yield total_batch_indexes.astype(int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.iterations"
      ],
      "metadata": {
        "id": "yODgabjHEY9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 500\n",
        "batch_size = 1\n",
        "\n",
        "train_sampler = BatchSampler(labels=train_labels, classes_per_it=N,\n",
        "                              support_size=support_size, query_size=query_size, iterations=iterations,\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "test_sampler = BatchSampler(labels=test_labels, classes_per_it=N,\n",
        "                              support_size=support_size, query_size=query_size, iterations=iterations,\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_sampler)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_sampler)"
      ],
      "metadata": {
        "id": "vkaRutIUPF4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n"
      ],
      "metadata": {
        "id": "xnvAPmPmPh92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels, momentum=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "class Feature_extractor(nn.Module):\n",
        "    '''\n",
        "    source: https://github.com/jakesnell/prototypical-networks/blob/f0c48808e496989d01db59f86d4449d7aee9ab0c/protonets/models/few_shot.py#L62-L84\n",
        "    '''\n",
        "    def __init__(self, x_dim=1, hid_dim=64):\n",
        "        super(Feature_extractor, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            conv_block(x_dim, hid_dim),\n",
        "            conv_block(hid_dim, hid_dim),\n",
        "            conv_block(hid_dim, hid_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)"
      ],
      "metadata": {
        "id": "eY3brHqVVXdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "feature_extractor = Feature_extractor()\n",
        "feature_extractor = feature_extractor.to(device)\n",
        "feature_extractor.load_state_dict(torch.load('./pretrained_model.pt', map_location=device))"
      ],
      "metadata": {
        "id": "QEucQ0e669mc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8f80b1-07cf-43a2-814b-6924e1b87cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Learner(nn.Module):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Learner, self).__init__()\n",
        "\n",
        "        # this dict contains all tensors needed to be optimized\n",
        "        self.vars = nn.ParameterList()\n",
        "        # running_mean and running_var\n",
        "        self.vars_bn = nn.ParameterList()\n",
        "\n",
        "        self.config = kwargs['config']\n",
        "\n",
        "        for i, (name, param) in enumerate(self.config):\n",
        "\n",
        "            if name is 'conv2d':\n",
        "                w = nn.Parameter(torch.ones(*param[:4]))\n",
        "                torch.nn.init.kaiming_normal_(w)\n",
        "                self.vars.append(w)\n",
        "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
        "\n",
        "            elif name is 'bn':\n",
        "                w = nn.Parameter(torch.ones(param[0]))\n",
        "                self.vars.append(w)\n",
        "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
        "                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)\n",
        "                running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)\n",
        "                self.vars_bn.extend([running_mean, running_var])\n",
        "\n",
        "            elif name is 'linear':\n",
        "                w = nn.Parameter(torch.ones(*param))\n",
        "                torch.nn.init.kaiming_normal_(w)\n",
        "                self.vars.append(w)\n",
        "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
        "\n",
        "\n",
        "    def forward(self, x, vars=None, bn_training=True):\n",
        "\n",
        "        if vars is None:\n",
        "            vars = self.vars\n",
        "        idx = 0\n",
        "        bn_idx = 0\n",
        "\n",
        "        for name, param in self.config:\n",
        "            if name is 'conv2d':\n",
        "                w, b = vars[idx], vars[idx + 1]\n",
        "                x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\n",
        "                idx += 2\n",
        "            elif name is 'bn':\n",
        "                w, b = vars[idx], vars[idx + 1]\n",
        "                running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx+1]\n",
        "                x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)\n",
        "                idx += 2\n",
        "                bn_idx += 2\n",
        "            elif name is 'linear':\n",
        "                w, b = vars[idx], vars[idx + 1]\n",
        "                x = F.linear(x, w, b)\n",
        "                idx += 2\n",
        "            elif name is 'flatten':\n",
        "                x = x.view(x.size(0), -1)\n",
        "            elif name is 'relu':\n",
        "                x = F.relu(x, inplace=param[0])\n",
        "            elif name is 'max_pool2d':\n",
        "                x = F.max_pool2d(x, param[0], param[1], param[2])\n",
        "\n",
        "        # Check correctness\n",
        "        assert idx == len(vars)\n",
        "        assert bn_idx == len(self.vars_bn)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def zero_grad(self, vars=None):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if vars is None:\n",
        "                for p in self.vars:\n",
        "                    if p.grad is not None:\n",
        "                        p.grad.zero_()\n",
        "            else:\n",
        "                for p in vars:\n",
        "                    if p.grad is not None:\n",
        "                        p.grad.zero_()\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.vars"
      ],
      "metadata": {
        "id": "S7QX9qUnBLSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Meta(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        super(Meta, self).__init__()\n",
        "\n",
        "        self.config = kwargs['config']\n",
        "        self.update_lr = kwargs['inner_lr']\n",
        "        self.finetune_steps = kwargs['finetune_steps']\n",
        "        self.net = Learner(config=self.config)\n",
        "\n",
        "\n",
        "    def forward(self, x_support, y_support, x_query, fc=True):\n",
        "        batch_size, support_size, c, h, w = x_support.shape\n",
        "        _, query_size, _, _, _ = x_query.shape\n",
        "        n_way = len(torch.unique(y_support[0]))\n",
        "        outputs = torch.zeros((batch_size, query_size, n_way)).to(x_support.device)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            x = x_support[i]                    # x: (s, c, h, w)\n",
        "            y = y_support[i]                    # y: (s, )\n",
        "            weights = self.net.parameters()\n",
        "            for j in range(0, self.finetune_steps):\n",
        "                features = x\n",
        "                if fc:\n",
        "                    features = feature_extractor(x)\n",
        "                logits = self.net(features, weights, bn_training=True)\n",
        "                loss = F.cross_entropy(logits, y)\n",
        "                # print(j, loss)\n",
        "                grad = torch.autograd.grad(loss, weights)\n",
        "                weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, weights)))\n",
        "            features = x_query[i]\n",
        "            if fc:\n",
        "                features = feature_extractor(x_query[i])\n",
        "            logits_q = self.net(features, weights, bn_training=True)\n",
        "            outputs[i, :, :] = logits_q\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "-xcU9vL05ks-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Feature Extractor"
      ],
      "metadata": {
        "id": "qgbrX1ykMYre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "si_-sVXgbYbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def train(model, epochs, train_dataloader):\n",
        "\n",
        "    meta_optim = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    log_every_iter = 50\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        running_loss = 0\n",
        "        running_accuracy = 0\n",
        "        for i, data in enumerate(train_dataloader):\n",
        "\n",
        "            # Prepare input data\n",
        "            x, y = data\n",
        "            img_size = x.shape[1:]\n",
        "            x = x.reshape((batch_size, -1, *img_size))\n",
        "            y = y.reshape((batch_size, -1))\n",
        "            for j in range(y.shape[0]):\n",
        "                _, new_labels = torch.unique(y[j, :], return_inverse=True)\n",
        "                y[j, :] = new_labels\n",
        "\n",
        "            # To GPU\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            supports = x[:, :N*support_size, :, :, :]\n",
        "            queries = x[:, N*support_size:, :, :, :]\n",
        "            support_labels = y[:, :N*support_size]\n",
        "            query_labels = y[:, N*support_size:]\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = model(supports, support_labels, queries)\n",
        "            loss = F.cross_entropy(outputs.reshape((-1, N)), query_labels.reshape((-1, )))\n",
        "            with torch.no_grad():\n",
        "                preds = outputs.argmax(dim=2)\n",
        "                acc = ((preds == query_labels) * 1.0).mean()\n",
        "\n",
        "            # Backward and parameter update\n",
        "            meta_optim.zero_grad()\n",
        "            loss.backward()\n",
        "            meta_optim.step()\n",
        "\n",
        "            # Keep metrics\n",
        "            running_loss += loss.item()\n",
        "            running_accuracy += acc.item()\n",
        "\n",
        "            if i % log_every_iter == log_every_iter - 1:\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss/log_every_iter:.3f}, \\\n",
        "                        accuracy = {running_accuracy/log_every_iter:.3f}')\n",
        "                running_loss = 0.0\n",
        "                running_accuracy = 0.0\n",
        "\n",
        "def test(model, test_dataloader):\n",
        "\n",
        "    running_accuracy = 0\n",
        "    model.eval()\n",
        "    for data in test_dataloader:\n",
        "\n",
        "        # Prepare input data\n",
        "        x, y = data\n",
        "        img_size = x.shape[1:]\n",
        "        x = x.reshape((batch_size, -1, *img_size))\n",
        "        y = y.reshape((batch_size, -1))\n",
        "        for j in range(y.shape[0]):\n",
        "            _, new_labels = torch.unique(y[j, :], return_inverse=True)\n",
        "            y[j, :] = new_labels\n",
        "\n",
        "        # To GPU\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        supports = x[:, :N*support_size, :, :, :]\n",
        "        queries = x[:, N*support_size:, :, :, :]\n",
        "        support_labels = y[:, :N*support_size]\n",
        "        query_labels = y[:, N*support_size:]\n",
        "\n",
        "        # Forward Pass\n",
        "        outputs = model(supports, support_labels, queries)\n",
        "        preds = outputs.argmax(dim=2)\n",
        "        acc = ((preds == query_labels) * 1.0).mean()\n",
        "\n",
        "        # Keep metrics\n",
        "        running_accuracy += acc.item()\n",
        "\n",
        "    acc = running_accuracy / len(test_dataloader)\n",
        "    print('')\n",
        "    print(f'Test accuracy is : {acc:.3f}')\n",
        "    print('------------------------------------------')\n",
        "    return acc\n",
        "\n",
        "# -------------------------------------------\n",
        "\n",
        "epochs = 5\n",
        "config = [\n",
        "    ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
        "    ('bn', [64]),\n",
        "    ('relu', [True]),\n",
        "    ('max_pool2d', [2, 2, 0]),\n",
        "    ('flatten', []),\n",
        "    ('linear', [N, 64])\n",
        "]\n",
        "update_steps = [1, 2, 3]\n",
        "accuracies = []\n",
        "print('Number of iterations in each epoch: {}'.format(len(train_dataloader)))\n",
        "print('Number of testing iterations: {}'.format(len(test_dataloader)))\n",
        "print('Training started ...')\n",
        "for us in update_steps:\n",
        "    print(f'--------------- number inner updates = {us} ---------------')\n",
        "    model = Meta(config=config, inner_lr=4e-1, finetune_steps=us).to(device)\n",
        "    train(model=model, epochs=epochs, train_dataloader=train_dataloader)\n",
        "    acc = test(model=model, test_dataloader=test_dataloader)\n",
        "    accuracies.append(acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn8Nlc0rzPO3",
        "outputId": "ae43b63b-0193-454a-d498-92ea79a8d260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations in each epoch: 500\n",
            "Number of testing iterations: 500\n",
            "Training started ...\n",
            "--------------- number inner updates = 1 ---------------\n",
            "[1,    50] loss: 3.144,                         accuracy = 0.503\n",
            "[1,   100] loss: 1.634,                         accuracy = 0.527\n",
            "[1,   150] loss: 1.378,                         accuracy = 0.483\n",
            "[1,   200] loss: 1.332,                         accuracy = 0.523\n",
            "[1,   250] loss: 1.285,                         accuracy = 0.571\n",
            "[1,   300] loss: 1.267,                         accuracy = 0.577\n",
            "[1,   350] loss: 1.220,                         accuracy = 0.631\n",
            "[1,   400] loss: 1.221,                         accuracy = 0.622\n",
            "[1,   450] loss: 1.191,                         accuracy = 0.626\n",
            "[1,   500] loss: 1.171,                         accuracy = 0.651\n",
            "[2,    50] loss: 1.107,                         accuracy = 0.666\n",
            "[2,   100] loss: 1.128,                         accuracy = 0.643\n",
            "[2,   150] loss: 1.089,                         accuracy = 0.666\n",
            "[2,   200] loss: 1.061,                         accuracy = 0.643\n",
            "[2,   250] loss: 1.012,                         accuracy = 0.692\n",
            "[2,   300] loss: 0.927,                         accuracy = 0.715\n",
            "[2,   350] loss: 0.938,                         accuracy = 0.710\n",
            "[2,   400] loss: 0.906,                         accuracy = 0.740\n",
            "[2,   450] loss: 0.845,                         accuracy = 0.738\n",
            "[2,   500] loss: 0.833,                         accuracy = 0.760\n",
            "[3,    50] loss: 0.878,                         accuracy = 0.709\n",
            "[3,   100] loss: 0.833,                         accuracy = 0.746\n",
            "[3,   150] loss: 0.856,                         accuracy = 0.727\n",
            "[3,   200] loss: 0.810,                         accuracy = 0.741\n",
            "[3,   250] loss: 0.813,                         accuracy = 0.733\n",
            "[3,   300] loss: 0.857,                         accuracy = 0.722\n",
            "[3,   350] loss: 0.824,                         accuracy = 0.738\n",
            "[3,   400] loss: 0.791,                         accuracy = 0.747\n",
            "[3,   450] loss: 0.806,                         accuracy = 0.745\n",
            "[3,   500] loss: 0.795,                         accuracy = 0.748\n",
            "[4,    50] loss: 0.800,                         accuracy = 0.738\n",
            "[4,   100] loss: 0.827,                         accuracy = 0.717\n",
            "[4,   150] loss: 0.796,                         accuracy = 0.749\n",
            "[4,   200] loss: 0.812,                         accuracy = 0.759\n",
            "[4,   250] loss: 0.845,                         accuracy = 0.723\n",
            "[4,   300] loss: 0.757,                         accuracy = 0.765\n",
            "[4,   350] loss: 0.763,                         accuracy = 0.773\n",
            "[4,   400] loss: 0.805,                         accuracy = 0.738\n",
            "[4,   450] loss: 0.796,                         accuracy = 0.744\n",
            "[4,   500] loss: 0.795,                         accuracy = 0.751\n",
            "[5,    50] loss: 0.798,                         accuracy = 0.745\n",
            "[5,   100] loss: 0.817,                         accuracy = 0.714\n",
            "[5,   150] loss: 0.747,                         accuracy = 0.760\n",
            "[5,   200] loss: 0.816,                         accuracy = 0.709\n",
            "[5,   250] loss: 0.767,                         accuracy = 0.750\n",
            "[5,   300] loss: 0.793,                         accuracy = 0.741\n",
            "[5,   350] loss: 0.819,                         accuracy = 0.734\n",
            "[5,   400] loss: 0.802,                         accuracy = 0.732\n",
            "[5,   450] loss: 0.852,                         accuracy = 0.715\n",
            "[5,   500] loss: 0.867,                         accuracy = 0.701\n",
            "\n",
            "Test accuracy is : 0.629\n",
            "------------------------------------------\n",
            "--------------- number inner updates = 2 ---------------\n",
            "[1,    50] loss: 2.760,                         accuracy = 0.503\n",
            "[1,   100] loss: 1.455,                         accuracy = 0.515\n",
            "[1,   150] loss: 1.089,                         accuracy = 0.613\n",
            "[1,   200] loss: 1.298,                         accuracy = 0.525\n",
            "[1,   250] loss: 1.059,                         accuracy = 0.622\n",
            "[1,   300] loss: 1.173,                         accuracy = 0.587\n",
            "[1,   350] loss: 0.916,                         accuracy = 0.705\n",
            "[1,   400] loss: 0.805,                         accuracy = 0.750\n",
            "[1,   450] loss: 1.131,                         accuracy = 0.594\n",
            "[1,   500] loss: 0.939,                         accuracy = 0.646\n",
            "[2,    50] loss: 0.838,                         accuracy = 0.711\n",
            "[2,   100] loss: 0.708,                         accuracy = 0.810\n",
            "[2,   150] loss: 0.651,                         accuracy = 0.801\n",
            "[2,   200] loss: 1.026,                         accuracy = 0.682\n",
            "[2,   250] loss: 0.986,                         accuracy = 0.644\n",
            "[2,   300] loss: 0.851,                         accuracy = 0.690\n",
            "[2,   350] loss: 0.886,                         accuracy = 0.695\n",
            "[2,   400] loss: 0.855,                         accuracy = 0.707\n",
            "[2,   450] loss: 0.852,                         accuracy = 0.692\n",
            "[2,   500] loss: 0.782,                         accuracy = 0.709\n",
            "[3,    50] loss: 0.776,                         accuracy = 0.743\n",
            "[3,   100] loss: 0.718,                         accuracy = 0.785\n",
            "[3,   150] loss: 0.790,                         accuracy = 0.725\n",
            "[3,   200] loss: 0.701,                         accuracy = 0.775\n",
            "[3,   250] loss: 0.697,                         accuracy = 0.780\n",
            "[3,   300] loss: 0.714,                         accuracy = 0.768\n",
            "[3,   350] loss: 0.707,                         accuracy = 0.783\n",
            "[3,   400] loss: 0.711,                         accuracy = 0.793\n",
            "[3,   450] loss: 0.733,                         accuracy = 0.751\n",
            "[3,   500] loss: 0.708,                         accuracy = 0.782\n",
            "[4,    50] loss: 0.722,                         accuracy = 0.766\n",
            "[4,   100] loss: 0.688,                         accuracy = 0.802\n",
            "[4,   150] loss: 0.694,                         accuracy = 0.756\n",
            "[4,   200] loss: 0.645,                         accuracy = 0.808\n",
            "[4,   250] loss: 0.650,                         accuracy = 0.799\n",
            "[4,   300] loss: 0.650,                         accuracy = 0.781\n",
            "[4,   350] loss: 0.646,                         accuracy = 0.797\n",
            "[4,   400] loss: 0.641,                         accuracy = 0.802\n",
            "[4,   450] loss: 0.659,                         accuracy = 0.791\n",
            "[4,   500] loss: 0.674,                         accuracy = 0.790\n",
            "[5,    50] loss: 0.674,                         accuracy = 0.763\n",
            "[5,   100] loss: 0.636,                         accuracy = 0.802\n",
            "[5,   150] loss: 0.607,                         accuracy = 0.798\n",
            "[5,   200] loss: 0.662,                         accuracy = 0.768\n",
            "[5,   250] loss: 0.629,                         accuracy = 0.784\n",
            "[5,   300] loss: 0.645,                         accuracy = 0.786\n",
            "[5,   350] loss: 0.617,                         accuracy = 0.801\n",
            "[5,   400] loss: 0.611,                         accuracy = 0.791\n",
            "[5,   450] loss: 0.652,                         accuracy = 0.771\n",
            "[5,   500] loss: 0.638,                         accuracy = 0.795\n",
            "\n",
            "Test accuracy is : 0.713\n",
            "------------------------------------------\n",
            "--------------- number inner updates = 3 ---------------\n",
            "[1,    50] loss: 2.647,                         accuracy = 0.512\n",
            "[1,   100] loss: 1.041,                         accuracy = 0.638\n",
            "[1,   150] loss: 0.776,                         accuracy = 0.717\n",
            "[1,   200] loss: 0.734,                         accuracy = 0.755\n",
            "[1,   250] loss: 0.714,                         accuracy = 0.774\n",
            "[1,   300] loss: 0.671,                         accuracy = 0.795\n",
            "[1,   350] loss: 0.664,                         accuracy = 0.797\n",
            "[1,   400] loss: 0.658,                         accuracy = 0.831\n",
            "[1,   450] loss: 0.656,                         accuracy = 0.819\n",
            "[1,   500] loss: 0.612,                         accuracy = 0.841\n",
            "[2,    50] loss: 0.555,                         accuracy = 0.871\n",
            "[2,   100] loss: 0.565,                         accuracy = 0.865\n",
            "[2,   150] loss: 0.497,                         accuracy = 0.889\n",
            "[2,   200] loss: 0.533,                         accuracy = 0.857\n",
            "[2,   250] loss: 0.527,                         accuracy = 0.882\n",
            "[2,   300] loss: 0.436,                         accuracy = 0.905\n",
            "[2,   350] loss: 0.486,                         accuracy = 0.881\n",
            "[2,   400] loss: 0.420,                         accuracy = 0.907\n",
            "[2,   450] loss: 0.438,                         accuracy = 0.897\n",
            "[2,   500] loss: 0.400,                         accuracy = 0.909\n",
            "[3,    50] loss: 0.414,                         accuracy = 0.897\n",
            "[3,   100] loss: 0.400,                         accuracy = 0.904\n",
            "[3,   150] loss: 0.425,                         accuracy = 0.880\n",
            "[3,   200] loss: 0.446,                         accuracy = 0.880\n",
            "[3,   250] loss: 0.383,                         accuracy = 0.905\n",
            "[3,   300] loss: 0.409,                         accuracy = 0.886\n",
            "[3,   350] loss: 0.378,                         accuracy = 0.886\n",
            "[3,   400] loss: 0.408,                         accuracy = 0.881\n",
            "[3,   450] loss: 0.370,                         accuracy = 0.914\n",
            "[3,   500] loss: 0.353,                         accuracy = 0.903\n",
            "[4,    50] loss: 0.388,                         accuracy = 0.890\n",
            "[4,   100] loss: 0.364,                         accuracy = 0.894\n",
            "[4,   150] loss: 0.354,                         accuracy = 0.903\n",
            "[4,   200] loss: 0.420,                         accuracy = 0.886\n",
            "[4,   250] loss: 0.345,                         accuracy = 0.911\n",
            "[4,   300] loss: 0.348,                         accuracy = 0.917\n",
            "[4,   350] loss: 0.330,                         accuracy = 0.918\n",
            "[4,   400] loss: 0.366,                         accuracy = 0.905\n",
            "[4,   450] loss: 0.372,                         accuracy = 0.901\n",
            "[4,   500] loss: 0.350,                         accuracy = 0.906\n",
            "[5,    50] loss: 0.329,                         accuracy = 0.923\n",
            "[5,   100] loss: 0.353,                         accuracy = 0.894\n",
            "[5,   150] loss: 0.360,                         accuracy = 0.900\n",
            "[5,   200] loss: 0.357,                         accuracy = 0.907\n",
            "[5,   250] loss: 0.357,                         accuracy = 0.894\n",
            "[5,   300] loss: 0.352,                         accuracy = 0.896\n",
            "[5,   350] loss: 0.337,                         accuracy = 0.910\n",
            "[5,   400] loss: 0.324,                         accuracy = 0.916\n",
            "[5,   450] loss: 0.365,                         accuracy = 0.897\n",
            "[5,   500] loss: 0.349,                         accuracy = 0.896\n",
            "\n",
            "Test accuracy is : 0.851\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ],
      "metadata": {
        "id": "E5X-lOSBN15I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot accuracy of meta-test phase based on inner loop update parameter."
      ],
      "metadata": {
        "id": "5hDTwQqmNnX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([1, 2, 3], accuracies)\n",
        "plt.scatter([1, 2, 3], accuracies)\n",
        "plt.xlabel('Number of updates')\n",
        "plt.ylabel('Accuracy on test data')"
      ],
      "metadata": {
        "id": "4FDQZ0WoNCOS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "227b7ea5-29a0-4e5d-9d09-59f77a919039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy on test data')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Without Feature Extractor"
      ],
      "metadata": {
        "id": "ZTILlHKIIGfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train\n"
      ],
      "metadata": {
        "id": "NTDVBPWqOeNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def train_complete(model, epochs, train_dataloader):\n",
        "\n",
        "    meta_optim = optim.Adam(model.parameters(), lr=meta_outer_lr)\n",
        "    log_every_iter = 50\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        running_loss = 0\n",
        "        running_accuracy = 0\n",
        "        for i, data in enumerate(train_dataloader):\n",
        "\n",
        "            # Prepare input data\n",
        "            x, y = data\n",
        "            img_size = x.shape[1:]\n",
        "            x = x.reshape((batch_size, -1, *img_size))\n",
        "            y = y.reshape((batch_size, -1))\n",
        "            for j in range(y.shape[0]):\n",
        "                _, new_labels = torch.unique(y[j, :], return_inverse=True)\n",
        "                y[j, :] = new_labels\n",
        "\n",
        "            # To GPU\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            supports = x[:, :N*support_size, :, :, :]\n",
        "            queries = x[:, N*support_size:, :, :, :]\n",
        "            support_labels = y[:, :N*support_size]\n",
        "            query_labels = y[:, N*support_size:]\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = model(supports, support_labels, queries, fc=False)\n",
        "            loss = F.cross_entropy(outputs.reshape((-1, N)), query_labels.reshape((-1, )))\n",
        "            preds = outputs.argmax(dim=2)\n",
        "            acc = ((preds == query_labels) * 1.0).mean()\n",
        "\n",
        "            # Backward and parameter update\n",
        "            meta_optim.zero_grad()\n",
        "            loss.backward()\n",
        "            meta_optim.step()\n",
        "\n",
        "            # Keep metrics\n",
        "            running_loss += loss.item()\n",
        "            running_accuracy += acc.item()\n",
        "\n",
        "            if i % log_every_iter == log_every_iter - 1:\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss/log_every_iter:.3f}, \\\n",
        "                        accuracy = {running_accuracy/log_every_iter:.3f}')\n",
        "                running_loss = 0.0\n",
        "                running_accuracy = 0.0\n",
        "\n",
        "def test_complete(model, test_dataloader):\n",
        "\n",
        "    running_accuracy = 0\n",
        "    model.eval()\n",
        "    for data in test_dataloader:\n",
        "\n",
        "        # Prepare input data\n",
        "        x, y = data\n",
        "        img_size = x.shape[1:]\n",
        "        x = x.reshape((batch_size, -1, *img_size))\n",
        "        y = y.reshape((batch_size, -1))\n",
        "        for j in range(y.shape[0]):\n",
        "            _, new_labels = torch.unique(y[j, :], return_inverse=True)\n",
        "            y[j, :] = new_labels\n",
        "\n",
        "        # To GPU\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        supports = x[:, :N*support_size, :, :, :]\n",
        "        queries = x[:, N*support_size:, :, :, :]\n",
        "        support_labels = y[:, :N*support_size]\n",
        "        query_labels = y[:, N*support_size:]\n",
        "\n",
        "        # Forward Pass\n",
        "        outputs = model(supports, support_labels, queries, fc=False)\n",
        "        preds = outputs.argmax(dim=2)\n",
        "        acc = ((preds == query_labels) * 1.0).mean()\n",
        "\n",
        "        # Keep metrics\n",
        "        running_accuracy += acc.item()\n",
        "\n",
        "    acc = running_accuracy / len(test_dataloader)\n",
        "    print('')\n",
        "    print(f'Test accuracy is : {acc:.3f}')\n",
        "    print('------------------------------------------')\n",
        "    return acc\n",
        "\n",
        "epochs = 10\n",
        "config = [\n",
        "    ('conv2d', [64, 1, 3, 3, 1, 1]),\n",
        "    ('bn', [64]),\n",
        "    ('relu', [True]),\n",
        "    ('max_pool2d', [2, 2, 0]),\n",
        "    ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
        "    ('bn', [64]),\n",
        "    ('relu', [True]),\n",
        "    ('max_pool2d', [2, 2, 0]),\n",
        "    ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
        "    ('bn', [64]),\n",
        "    ('relu', [True]),\n",
        "    ('max_pool2d', [2, 2, 0]),\n",
        "    ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
        "    ('bn', [64]),\n",
        "    ('relu', [True]),\n",
        "    ('max_pool2d', [2, 2, 0]),\n",
        "    ('flatten', []),\n",
        "    ('linear', [N, 64])\n",
        "]\n",
        "print('Number of iterations in each epoch: {}'.format(len(train_dataloader)))\n",
        "print('Number of testing iterations: {}'.format(len(test_dataloader)))\n",
        "print('Training started ...')\n",
        "model = Meta(config=config, inner_lr=meta_inner_lr, finetune_steps=1).to(device)\n",
        "train_complete(model=model, epochs=epochs, train_dataloader=train_dataloader)\n",
        "acc = test_complete(model=model, test_dataloader=test_dataloader)"
      ],
      "metadata": {
        "id": "4ALu1EEDQ0RQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf70649-5404-4526-a337-d3b2ae6cff41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations in each epoch: 500\n",
            "Number of testing iterations: 500\n",
            "Training started ...\n",
            "[1,    50] loss: 2.686,                         accuracy = 0.372\n",
            "[1,   100] loss: 1.499,                         accuracy = 0.398\n",
            "[1,   150] loss: 1.456,                         accuracy = 0.428\n",
            "[1,   200] loss: 1.415,                         accuracy = 0.458\n",
            "[1,   250] loss: 1.401,                         accuracy = 0.454\n",
            "[1,   300] loss: 1.322,                         accuracy = 0.479\n",
            "[1,   350] loss: 1.290,                         accuracy = 0.521\n",
            "[1,   400] loss: 1.228,                         accuracy = 0.559\n",
            "[1,   450] loss: 1.173,                         accuracy = 0.574\n",
            "[1,   500] loss: 1.153,                         accuracy = 0.560\n",
            "[2,    50] loss: 1.070,                         accuracy = 0.612\n",
            "[2,   100] loss: 1.000,                         accuracy = 0.644\n",
            "[2,   150] loss: 0.961,                         accuracy = 0.655\n",
            "[2,   200] loss: 0.988,                         accuracy = 0.622\n",
            "[2,   250] loss: 0.977,                         accuracy = 0.639\n",
            "[2,   300] loss: 0.903,                         accuracy = 0.699\n",
            "[2,   350] loss: 0.926,                         accuracy = 0.671\n",
            "[2,   400] loss: 0.870,                         accuracy = 0.678\n",
            "[2,   450] loss: 0.852,                         accuracy = 0.709\n",
            "[2,   500] loss: 0.785,                         accuracy = 0.717\n",
            "[3,    50] loss: 0.803,                         accuracy = 0.694\n",
            "[3,   100] loss: 0.799,                         accuracy = 0.730\n",
            "[3,   150] loss: 0.727,                         accuracy = 0.751\n",
            "[3,   200] loss: 0.762,                         accuracy = 0.734\n",
            "[3,   250] loss: 0.716,                         accuracy = 0.739\n",
            "[3,   300] loss: 0.728,                         accuracy = 0.751\n",
            "[3,   350] loss: 0.709,                         accuracy = 0.736\n",
            "[3,   400] loss: 0.709,                         accuracy = 0.739\n",
            "[3,   450] loss: 0.704,                         accuracy = 0.754\n",
            "[3,   500] loss: 0.644,                         accuracy = 0.771\n",
            "[4,    50] loss: 0.728,                         accuracy = 0.736\n",
            "[4,   100] loss: 0.772,                         accuracy = 0.720\n",
            "[4,   150] loss: 0.641,                         accuracy = 0.781\n",
            "[4,   200] loss: 0.628,                         accuracy = 0.777\n",
            "[4,   250] loss: 0.625,                         accuracy = 0.787\n",
            "[4,   300] loss: 0.604,                         accuracy = 0.778\n",
            "[4,   350] loss: 0.642,                         accuracy = 0.768\n",
            "[4,   400] loss: 0.567,                         accuracy = 0.799\n",
            "[4,   450] loss: 0.643,                         accuracy = 0.769\n",
            "[4,   500] loss: 0.604,                         accuracy = 0.784\n",
            "[5,    50] loss: 0.631,                         accuracy = 0.770\n",
            "[5,   100] loss: 0.621,                         accuracy = 0.783\n",
            "[5,   150] loss: 0.572,                         accuracy = 0.802\n",
            "[5,   200] loss: 0.639,                         accuracy = 0.772\n",
            "[5,   250] loss: 0.577,                         accuracy = 0.799\n",
            "[5,   300] loss: 0.511,                         accuracy = 0.829\n",
            "[5,   350] loss: 0.576,                         accuracy = 0.804\n",
            "[5,   400] loss: 0.484,                         accuracy = 0.837\n",
            "[5,   450] loss: 0.534,                         accuracy = 0.813\n",
            "[5,   500] loss: 0.550,                         accuracy = 0.807\n",
            "[6,    50] loss: 0.529,                         accuracy = 0.819\n",
            "[6,   100] loss: 0.461,                         accuracy = 0.835\n",
            "[6,   150] loss: 0.511,                         accuracy = 0.820\n",
            "[6,   200] loss: 0.531,                         accuracy = 0.819\n",
            "[6,   250] loss: 0.518,                         accuracy = 0.823\n",
            "[6,   300] loss: 0.516,                         accuracy = 0.811\n",
            "[6,   350] loss: 0.469,                         accuracy = 0.831\n",
            "[6,   400] loss: 0.519,                         accuracy = 0.815\n",
            "[6,   450] loss: 0.469,                         accuracy = 0.850\n",
            "[6,   500] loss: 0.526,                         accuracy = 0.827\n",
            "[7,    50] loss: 0.508,                         accuracy = 0.819\n",
            "[7,   100] loss: 0.472,                         accuracy = 0.845\n",
            "[7,   150] loss: 0.473,                         accuracy = 0.834\n",
            "[7,   200] loss: 0.528,                         accuracy = 0.816\n",
            "[7,   250] loss: 0.487,                         accuracy = 0.833\n",
            "[7,   300] loss: 0.466,                         accuracy = 0.847\n",
            "[7,   350] loss: 0.477,                         accuracy = 0.835\n",
            "[7,   400] loss: 0.498,                         accuracy = 0.832\n",
            "[7,   450] loss: 0.431,                         accuracy = 0.864\n",
            "[7,   500] loss: 0.486,                         accuracy = 0.840\n",
            "[8,    50] loss: 0.468,                         accuracy = 0.856\n",
            "[8,   100] loss: 0.410,                         accuracy = 0.862\n",
            "[8,   150] loss: 0.504,                         accuracy = 0.835\n",
            "[8,   200] loss: 0.458,                         accuracy = 0.844\n",
            "[8,   250] loss: 0.493,                         accuracy = 0.839\n",
            "[8,   300] loss: 0.492,                         accuracy = 0.835\n",
            "[8,   350] loss: 0.455,                         accuracy = 0.846\n",
            "[8,   400] loss: 0.457,                         accuracy = 0.845\n",
            "[8,   450] loss: 0.407,                         accuracy = 0.867\n",
            "[8,   500] loss: 0.422,                         accuracy = 0.878\n",
            "[9,    50] loss: 0.484,                         accuracy = 0.847\n",
            "[9,   100] loss: 0.406,                         accuracy = 0.857\n",
            "[9,   150] loss: 0.455,                         accuracy = 0.846\n",
            "[9,   200] loss: 0.433,                         accuracy = 0.868\n",
            "[9,   250] loss: 0.485,                         accuracy = 0.841\n",
            "[9,   300] loss: 0.453,                         accuracy = 0.854\n",
            "[9,   350] loss: 0.457,                         accuracy = 0.845\n",
            "[9,   400] loss: 0.489,                         accuracy = 0.836\n",
            "[9,   450] loss: 0.393,                         accuracy = 0.867\n",
            "[9,   500] loss: 0.361,                         accuracy = 0.878\n",
            "[10,    50] loss: 0.365,                         accuracy = 0.886\n",
            "[10,   100] loss: 0.336,                         accuracy = 0.884\n",
            "[10,   150] loss: 0.477,                         accuracy = 0.838\n",
            "[10,   200] loss: 0.401,                         accuracy = 0.864\n",
            "[10,   250] loss: 0.367,                         accuracy = 0.880\n",
            "[10,   300] loss: 0.371,                         accuracy = 0.875\n",
            "[10,   350] loss: 0.403,                         accuracy = 0.859\n",
            "[10,   400] loss: 0.388,                         accuracy = 0.866\n",
            "[10,   450] loss: 0.469,                         accuracy = 0.837\n",
            "[10,   500] loss: 0.368,                         accuracy = 0.881\n",
            "\n",
            "Test accuracy is : 0.809\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report accuracy of meta-test phase."
      ],
      "metadata": {
        "id": "ylzdpCPfOZSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "id": "Pvsv3Xu1S_QI",
        "outputId": "41f59110-6cab-4083-e6d9-4d712c9af550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.809466686785221"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}